{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from src.data_management.data_reader import *\n",
    "from src.submission.submission_helper import *\n",
    "from src.model.models import *\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [\n",
    "        'owl',  # 1\n",
    "        'galaxy',  # 2\n",
    "        'lightning',  # 3\n",
    "        'wine-bottle',  # ...\n",
    "        't-shirt',\n",
    "        'waterfall',\n",
    "        'sword',\n",
    "        'school-bus',\n",
    "        'calculator',\n",
    "        'sheet-music',\n",
    "        'airplanes',\n",
    "        'lightbulb',\n",
    "        'skyscraper',\n",
    "        'mountain-bike',\n",
    "        'fireworks',\n",
    "        'computer-monitor',\n",
    "        'bear',\n",
    "        'grand-piano',\n",
    "        'kangaroo',\n",
    "        'laptop'\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAP BN MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1238 images belonging to 20 classes.\n",
      "Found 316 images belonging to 20 classes.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x25e72a96668>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading the data\n",
    "SEED = 400\n",
    "tf.random.set_seed(SEED)\n",
    "cwd = os.getcwd()\n",
    "root_path = os.path.join(cwd, \"..\")\n",
    "bs=32\n",
    "img_h = 410\n",
    "img_w = 410\n",
    "train_dataset, valid_dataset, train_gen, valid_gen = read_training_data(root_path, bs=bs, img_h=img_h, img_w=img_w)\n",
    "\n",
    "# Model init.\n",
    "model = GAPBN()\n",
    "model = model.get_model(bs=bs, img_h=img_h, img_w=img_w)\n",
    "model.load_weights(os.path.join(cwd, '..', 'report', 'classification_experiments', 'GAPBN_Nov20_18-25-02',\n",
    "                                    'retrain', '56','ckpts','cp_05.ckpt'))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 43s 4s/step - loss: 1.4527 - accuracy: 0.5506\n"
     ]
    }
   ],
   "source": [
    "eval_out = model.evaluate(x=valid_dataset,\n",
    "                          steps=len(valid_gen),\n",
    "                          verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.4527499616146087, 0.5506329]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 49s 5s/step\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n"
     ]
    }
   ],
   "source": [
    "# Calculating predictions for the validation set\n",
    "y_pred = model.predict(x=valid_dataset, steps=len(valid_gen), verbose=1)\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Getting the true classes\n",
    "iterator = iter(valid_dataset)\n",
    "\n",
    "y_valid = np.zeros((320, 20))\n",
    "i = 0\n",
    "for _ in range(len(valid_gen)):\n",
    "    print(i)\n",
    "    augmented_img, target = next(iterator)\n",
    "    for j in range(0, target.shape[0]):\n",
    "        y_valid[i] = target[j]\n",
    "        i+=1\n",
    "\n",
    "y_valid = y_valid[0:316]\n",
    "y_valid = np.argmax(y_valid, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.32      0.63      0.42        19\n",
      "           1       0.91      0.83      0.87        12\n",
      "           2       0.65      0.85      0.74        20\n",
      "           3       0.25      0.12      0.17        16\n",
      "           4       0.55      0.30      0.39        20\n",
      "           5       1.00      0.43      0.60        14\n",
      "           6       0.31      0.50      0.38        16\n",
      "           7       0.91      0.67      0.77        15\n",
      "           8       0.25      0.20      0.22        15\n",
      "           9       0.73      0.67      0.70        12\n",
      "          10       0.89      0.85      0.87        20\n",
      "          11       0.50      0.43      0.46        14\n",
      "          12       0.86      0.43      0.57        14\n",
      "          13       0.71      0.42      0.53        12\n",
      "          14       0.68      0.87      0.76        15\n",
      "          15       0.69      0.45      0.55        20\n",
      "          16       0.67      0.38      0.48        16\n",
      "          17       0.39      0.64      0.49        14\n",
      "          18       0.60      0.50      0.55        12\n",
      "          19       0.41      0.75      0.53        20\n",
      "\n",
      "    accuracy                           0.55       316\n",
      "   macro avg       0.61      0.55      0.55       316\n",
      "weighted avg       0.60      0.55      0.55       316\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_valid, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_valid_gapbn = y_valid\n",
    "y_pred_gapbn = y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAP2 MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1238 images belonging to 20 classes.\n",
      "Found 316 images belonging to 20 classes.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x25e21673a58>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading the data\n",
    "SEED = 400\n",
    "tf.random.set_seed(SEED)\n",
    "cwd = os.getcwd()\n",
    "root_path = os.path.join(cwd, \"..\")\n",
    "bs=32\n",
    "img_h = 440\n",
    "img_w = 440\n",
    "train_dataset, valid_dataset, train_gen, valid_gen = read_training_data(root_path, bs=bs, img_h=img_h, img_w=img_w)\n",
    "\n",
    "# Model init.\n",
    "model = GAP2()\n",
    "model = model.get_model(bs=bs, img_h=img_h, img_w=img_w)\n",
    "model.load_weights(os.path.join(cwd, '..', 'report', 'classification_experiments', 'GAP2_Nov19_17-36-08',\n",
    "                                    'retrain', '52','ckpts','cp_18.ckpt'))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 43s 4s/step\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n"
     ]
    }
   ],
   "source": [
    "# Calculating predictions for the validation set\n",
    "y_pred_gap2 = model.predict(x=valid_dataset, steps=len(valid_gen), verbose=1)\n",
    "y_pred_gap2 = np.argmax(y_pred_gap2, axis=1)\n",
    "\n",
    "# Getting the true classes\n",
    "iterator = iter(valid_dataset)\n",
    "\n",
    "y_valid_gap2 = np.zeros((320, 20))\n",
    "i = 0\n",
    "for _ in range(len(valid_gen)):\n",
    "    print(i)\n",
    "    augmented_img, target = next(iterator)\n",
    "    for j in range(0, target.shape[0]):\n",
    "        y_valid_gap2[i] = target[j]\n",
    "        i+=1\n",
    "\n",
    "y_valid_gap2 = y_valid_gap2[0:316]\n",
    "y_valid_gap2 = np.argmax(y_valid_gap2, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      0.42      0.44        19\n",
      "           1       0.83      0.83      0.83        12\n",
      "           2       0.76      0.80      0.78        20\n",
      "           3       0.35      0.38      0.36        16\n",
      "           4       0.57      0.60      0.59        20\n",
      "           5       0.64      0.64      0.64        14\n",
      "           6       0.55      0.38      0.44        16\n",
      "           7       0.87      0.87      0.87        15\n",
      "           8       0.47      0.47      0.47        15\n",
      "           9       0.78      0.58      0.67        12\n",
      "          10       1.00      0.95      0.97        20\n",
      "          11       0.31      0.36      0.33        14\n",
      "          12       0.47      0.64      0.55        14\n",
      "          13       0.64      0.58      0.61        12\n",
      "          14       0.67      0.67      0.67        15\n",
      "          15       0.43      0.80      0.56        20\n",
      "          16       0.33      0.31      0.32        16\n",
      "          17       0.82      0.64      0.72        14\n",
      "          18       0.43      0.25      0.32        12\n",
      "          19       0.64      0.45      0.53        20\n",
      "\n",
      "    accuracy                           0.59       316\n",
      "   macro avg       0.60      0.58      0.58       316\n",
      "weighted avg       0.60      0.59      0.59       316\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_valid_gap2, y_pred_gap2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1238 images belonging to 20 classes.\n",
      "Found 316 images belonging to 20 classes.\n",
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_48 (Conv2D)           (None, 256, 256, 16)      448       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_48 (MaxPooling (None, 128, 128, 16)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_49 (Conv2D)           (None, 128, 128, 32)      4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_49 (MaxPooling (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_50 (Conv2D)           (None, 64, 64, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_50 (MaxPooling (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_51 (Conv2D)           (None, 32, 32, 96)        55392     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_51 (MaxPooling (None, 16, 16, 96)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_52 (Conv2D)           (None, 16, 16, 128)       110720    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_52 (MaxPooling (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_53 (Conv2D)           (None, 8, 8, 160)         184480    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_53 (MaxPooling (None, 4, 4, 160)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_54 (Conv2D)           (None, 4, 4, 192)         276672    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_54 (MaxPooling (None, 2, 2, 192)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_55 (Conv2D)           (None, 2, 2, 224)         387296    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_55 (MaxPooling (None, 1, 1, 224)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 224)               0         \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 256)               57600     \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 20)                2580      \n",
      "=================================================================\n",
      "Total params: 1,131,220\n",
      "Trainable params: 1,131,220\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x25e255c8e10>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading the data\n",
    "SEED = 400\n",
    "tf.random.set_seed(SEED)\n",
    "cwd = os.getcwd()\n",
    "root_path = os.path.join(cwd, \"..\")\n",
    "bs=32\n",
    "img_h = 256\n",
    "img_w = 256\n",
    "train_dataset, valid_dataset, train_gen, valid_gen = read_training_data(root_path, bs=bs, img_h=img_h, img_w=img_w)\n",
    "\n",
    "# Model init.\n",
    "model = CNN1()\n",
    "model = model.get_model(bs=bs, img_h=img_h, img_w=img_w, num_classes=20, seed=1234)\n",
    "model.load_weights(os.path.join(cwd, '..', 'report', 'classification_experiments', 'CNN2_Nov18_10-36-06',\n",
    "                                    'retrain', '63','ckpts','cp_14.ckpt'))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 8s 822ms/step\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n"
     ]
    }
   ],
   "source": [
    "# Calculating predictions for the validation set\n",
    "y_pred_cnn = model.predict(x=valid_dataset, steps=len(valid_gen), verbose=1)\n",
    "y_pred_cnn = np.argmax(y_pred_cnn, axis=1)\n",
    "\n",
    "# Getting the true classes\n",
    "iterator = iter(valid_dataset)\n",
    "\n",
    "y_valid_cnn = np.zeros((320, 20))\n",
    "i = 0\n",
    "for _ in range(len(valid_gen)):\n",
    "    print(i)\n",
    "    augmented_img, target = next(iterator)\n",
    "    for j in range(0, target.shape[0]):\n",
    "        y_valid_cnn[i] = target[j]\n",
    "        i+=1\n",
    "\n",
    "y_valid_cnn = y_valid_cnn[0:316]\n",
    "y_valid_cnn = np.argmax(y_valid_cnn, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.63      0.63        19\n",
      "           1       1.00      0.50      0.67        12\n",
      "           2       0.68      0.75      0.71        20\n",
      "           3       0.19      0.31      0.24        16\n",
      "           4       0.64      0.35      0.45        20\n",
      "           5       0.80      0.57      0.67        14\n",
      "           6       0.47      0.50      0.48        16\n",
      "           7       0.79      0.73      0.76        15\n",
      "           8       0.36      0.33      0.34        15\n",
      "           9       0.86      0.50      0.63        12\n",
      "          10       0.80      0.80      0.80        20\n",
      "          11       0.29      0.36      0.32        14\n",
      "          12       0.50      0.64      0.56        14\n",
      "          13       0.80      0.67      0.73        12\n",
      "          14       0.72      0.87      0.79        15\n",
      "          15       0.44      0.80      0.57        20\n",
      "          16       0.38      0.19      0.25        16\n",
      "          17       0.56      0.71      0.63        14\n",
      "          18       0.75      0.50      0.60        12\n",
      "          19       0.53      0.45      0.49        20\n",
      "\n",
      "    accuracy                           0.56       316\n",
      "   macro avg       0.61      0.56      0.57       316\n",
      "weighted avg       0.60      0.56      0.56       316\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_valid_cnn, y_pred_cnn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMPARISON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.63      0.63        19\n",
      "           1       1.00      0.50      0.67        12\n",
      "           2       0.68      0.75      0.71        20\n",
      "           3       0.19      0.31      0.24        16\n",
      "           4       0.64      0.35      0.45        20\n",
      "           5       0.80      0.57      0.67        14\n",
      "           6       0.47      0.50      0.48        16\n",
      "           7       0.79      0.73      0.76        15\n",
      "           8       0.36      0.33      0.34        15\n",
      "           9       0.86      0.50      0.63        12\n",
      "          10       0.80      0.80      0.80        20\n",
      "          11       0.29      0.36      0.32        14\n",
      "          12       0.50      0.64      0.56        14\n",
      "          13       0.80      0.67      0.73        12\n",
      "          14       0.72      0.87      0.79        15\n",
      "          15       0.44      0.80      0.57        20\n",
      "          16       0.38      0.19      0.25        16\n",
      "          17       0.56      0.71      0.63        14\n",
      "          18       0.75      0.50      0.60        12\n",
      "          19       0.53      0.45      0.49        20\n",
      "\n",
      "    accuracy                           0.56       316\n",
      "   macro avg       0.61      0.56      0.57       316\n",
      "weighted avg       0.60      0.56      0.56       316\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_valid_cnn, y_pred_cnn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      0.42      0.44        19\n",
      "           1       0.83      0.83      0.83        12\n",
      "           2       0.76      0.80      0.78        20\n",
      "           3       0.35      0.38      0.36        16\n",
      "           4       0.57      0.60      0.59        20\n",
      "           5       0.64      0.64      0.64        14\n",
      "           6       0.55      0.38      0.44        16\n",
      "           7       0.87      0.87      0.87        15\n",
      "           8       0.47      0.47      0.47        15\n",
      "           9       0.78      0.58      0.67        12\n",
      "          10       1.00      0.95      0.97        20\n",
      "          11       0.31      0.36      0.33        14\n",
      "          12       0.47      0.64      0.55        14\n",
      "          13       0.64      0.58      0.61        12\n",
      "          14       0.67      0.67      0.67        15\n",
      "          15       0.43      0.80      0.56        20\n",
      "          16       0.33      0.31      0.32        16\n",
      "          17       0.82      0.64      0.72        14\n",
      "          18       0.43      0.25      0.32        12\n",
      "          19       0.64      0.45      0.53        20\n",
      "\n",
      "    accuracy                           0.59       316\n",
      "   macro avg       0.60      0.58      0.58       316\n",
      "weighted avg       0.60      0.59      0.59       316\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_valid_gap2, y_pred_gap2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.32      0.63      0.42        19\n",
      "           1       0.91      0.83      0.87        12\n",
      "           2       0.65      0.85      0.74        20\n",
      "           3       0.25      0.12      0.17        16\n",
      "           4       0.55      0.30      0.39        20\n",
      "           5       1.00      0.43      0.60        14\n",
      "           6       0.31      0.50      0.38        16\n",
      "           7       0.91      0.67      0.77        15\n",
      "           8       0.25      0.20      0.22        15\n",
      "           9       0.73      0.67      0.70        12\n",
      "          10       0.89      0.85      0.87        20\n",
      "          11       0.50      0.43      0.46        14\n",
      "          12       0.86      0.43      0.57        14\n",
      "          13       0.71      0.42      0.53        12\n",
      "          14       0.68      0.87      0.76        15\n",
      "          15       0.69      0.45      0.55        20\n",
      "          16       0.67      0.38      0.48        16\n",
      "          17       0.39      0.64      0.49        14\n",
      "          18       0.60      0.50      0.55        12\n",
      "          19       0.41      0.75      0.53        20\n",
      "\n",
      "    accuracy                           0.55       316\n",
      "   macro avg       0.61      0.55      0.55       316\n",
      "weighted avg       0.60      0.55      0.55       316\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_valid_gapbn, y_pred_gapbn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensembling the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class NetworkAverageEnsemble(object):\n",
    "    def __init__(self, model_list, num_items_to_predict):\n",
    "        self.model_list = model_list\n",
    "        self.num_models = len(model_list)\n",
    "        self.num_items_to_predict = num_items_to_predict\n",
    "\n",
    "    def predict(self, x=None, steps=None, verbose=1, num_classes=20):\n",
    "        predictions = np.zeros((self.num_models, self.num_items_to_predict, num_classes))\n",
    "\n",
    "        # Computing predictions\n",
    "        for i, model in enumerate(self.model_list):\n",
    "            predictions[i] = model.predict(x=x[i], steps=steps, verbose=verbose)\n",
    "\n",
    "        # Averaging predictions\n",
    "        y_pred = np.average(predictions, axis=0)\n",
    "\n",
    "        return y_pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1238 images belonging to 20 classes.\n",
      "Found 316 images belonging to 20 classes.\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_28 (Conv2D)           (None, 256, 256, 16)      448       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_28 (MaxPooling (None, 128, 128, 16)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_29 (Conv2D)           (None, 128, 128, 32)      4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_29 (MaxPooling (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_30 (Conv2D)           (None, 64, 64, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_30 (MaxPooling (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_31 (Conv2D)           (None, 32, 32, 96)        55392     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_31 (MaxPooling (None, 16, 16, 96)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_32 (Conv2D)           (None, 16, 16, 128)       110720    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_32 (MaxPooling (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_33 (Conv2D)           (None, 8, 8, 160)         184480    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_33 (MaxPooling (None, 4, 4, 160)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_34 (Conv2D)           (None, 4, 4, 192)         276672    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_34 (MaxPooling (None, 2, 2, 192)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_35 (Conv2D)           (None, 2, 2, 224)         387296    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_35 (MaxPooling (None, 1, 1, 224)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 224)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 256)               57600     \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 20)                2580      \n",
      "=================================================================\n",
      "Total params: 1,131,220\n",
      "Trainable params: 1,131,220\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x1d32b546f60>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs=32\n",
    "img_h = 256\n",
    "img_w = 256\n",
    "\n",
    "train_dataset, valid_dataset_cnn, train_gen, valid_gen_cnn = read_training_data(root_path, bs=bs, img_h=img_h, img_w=img_w)\n",
    "\n",
    "\n",
    "model_cnn = CNN1()\n",
    "model_cnn = model_cnn.get_model(bs=bs, img_h=img_h, img_w=img_w, num_classes=20,seed=SEED)\n",
    "\n",
    "model_cnn.load_weights(os.path.join(cwd, '..', 'report', 'classification_experiments', 'CNN2_Nov18_10-36-06',\n",
    "                                    'retrain', '63','ckpts','cp_14.ckpt'))  # use this if you want to restore saved training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1238 images belonging to 20 classes.\n",
      "Found 316 images belonging to 20 classes.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x1d32abd4c18>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SEED = 400\n",
    "tf.random.set_seed(SEED)\n",
    "cwd = os.getcwd()\n",
    "root_path = os.path.join(cwd, \"..\")\n",
    "\n",
    "\n",
    "bs=32\n",
    "img_h = 410\n",
    "img_w = 410\n",
    "\n",
    "train_dataset, valid_dataset_gapbn, train_gen, valid_gen_gapbn = read_training_data(root_path, bs=bs, img_h=img_h, img_w=img_w)\n",
    "\n",
    "# Model init.\n",
    "model_gapbn = GAPBN()\n",
    "model_gapbn = model_gapbn.get_model(bs=bs, img_h=img_h, img_w=img_w)\n",
    "model_gapbn.load_weights(os.path.join(cwd, '..', 'report', 'classification_experiments', 'GAPBN_Nov20_18-25-02',\n",
    "                                    'retrain', '56','ckpts','cp_05.ckpt'))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1238 images belonging to 20 classes.\n",
      "Found 316 images belonging to 20 classes.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x1d32afc5f60>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_h = 440\n",
    "img_w = 440\n",
    "\n",
    "train_dataset, valid_dataset_gap2, train_gen, valid_gen_gap2 = read_training_data(root_path, bs=bs, img_h=img_h, img_w=img_w)\n",
    "\n",
    "\n",
    "# Model init.\n",
    "model_gap2 = GAP2()\n",
    "model_gap2 = model_gap2.get_model(bs=bs, img_h=img_h, img_w=img_w)\n",
    "model_gap2.load_weights(os.path.join(cwd, '..', 'report', 'classification_experiments', 'GAP2_Nov19_17-36-08',\n",
    "                                    'retrain', '52','ckpts','cp_18.ckpt'))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ensambles = []\n",
    "model_ensambles.append(model_gap2)\n",
    "model_ensambles.append(model_cnn)\n",
    "model_ensambles.append(model_gapbn)\n",
    "\n",
    "data_list = []\n",
    "data_list.append(valid_dataset_gap2)\n",
    "data_list.append(valid_dataset_cnn)\n",
    "data_list.append(valid_dataset_gapbn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensamble = NetworkAverageEnsemble(model_ensambles, 316)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 26s 3s/step\n",
      "10/10 [==============================] - 5s 470ms/step\n",
      "10/10 [==============================] - 25s 3s/step\n"
     ]
    }
   ],
   "source": [
    "y_pred_en = ensamble.predict(x=data_list, steps=len(valid_gen_gapbn), verbose=1)\n",
    "y_pred_en = np.argmax(y_pred_en, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n"
     ]
    }
   ],
   "source": [
    "# Getting the true classes\n",
    "iterator = iter(valid_dataset_gapbn)\n",
    "\n",
    "y_valid = np.zeros((320, 20))\n",
    "i = 0\n",
    "for _ in range(len(valid_gen_gapbn)):\n",
    "    print(i)\n",
    "    augmented_img, target = next(iterator)\n",
    "    for j in range(0, target.shape[0]):\n",
    "        y_valid[i] = target[j]\n",
    "        i+=1\n",
    "\n",
    "y_valid = y_valid[0:316]\n",
    "y_valid = np.argmax(y_valid, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.38      0.53      0.44        19\n",
      "           1       0.57      0.67      0.62        12\n",
      "           2       0.50      0.75      0.60        20\n",
      "           3       0.27      0.19      0.22        16\n",
      "           4       0.60      0.30      0.40        20\n",
      "           5       0.73      0.57      0.64        14\n",
      "           6       0.44      0.44      0.44        16\n",
      "           7       0.62      0.53      0.57        15\n",
      "           8       0.11      0.07      0.08        15\n",
      "           9       0.70      0.58      0.64        12\n",
      "          10       0.61      0.85      0.71        20\n",
      "          11       0.23      0.21      0.22        14\n",
      "          12       0.41      0.50      0.45        14\n",
      "          13       0.55      0.50      0.52        12\n",
      "          14       0.62      0.87      0.72        15\n",
      "          15       0.36      0.50      0.42        20\n",
      "          16       0.67      0.12      0.21        16\n",
      "          17       0.50      0.71      0.59        14\n",
      "          18       0.40      0.17      0.24        12\n",
      "          19       0.30      0.30      0.30        20\n",
      "\n",
      "    accuracy                           0.47       316\n",
      "   macro avg       0.48      0.47      0.45       316\n",
      "weighted avg       0.47      0.47      0.45       316\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_valid, y_pred_en))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resnet50v2 - RSN1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x19682725cf8>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cwd = os.getcwd()\n",
    "num_classes=20\n",
    "bs=32\n",
    "img_h = 256\n",
    "img_w = 256\n",
    "model_name = \"RSN1\"\n",
    "resnet = tf.keras.applications.resnet_v2.ResNet50V2(include_top=False, weights='imagenet', input_tensor=None,\n",
    "                                                input_shape=(img_w, img_h, 3), pooling='avg', classes=1000)\n",
    "resnet.trainable = False\n",
    "model = tf.keras.Sequential()\n",
    "model.add(resnet)\n",
    "model.add(tf.keras.layers.Dense(units=256, activation='elu'))\n",
    "model.add(tf.keras.layers.Dropout(rate=0.5))\n",
    "model.add(tf.keras.layers.Dense(units=num_classes, activation='softmax'))\n",
    "\n",
    "loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "metrics = ['accuracy']\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "\n",
    "model.load_weights(os.path.join(cwd, '..', 'report', 'classification_experiments', 'RSN1_Nov21_22-59-47',\n",
    "                                    'ckpts', 'cp_08.ckpt'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1238 images belonging to 20 classes.\n",
      "Found 316 images belonging to 20 classes.\n"
     ]
    }
   ],
   "source": [
    "root_path = os.path.join(cwd, \"..\")\n",
    "train_dataset, valid_dataset, train_gen, valid_gen = read_training_data(root_path, bs=bs, img_w=img_w, img_h=img_h,\n",
    "                                                                            to_rescale=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 84s 8s/step\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n"
     ]
    }
   ],
   "source": [
    "# Calculating predictions for the validation set\n",
    "y_pred = model.predict(x=valid_dataset, steps=len(valid_gen), verbose=1)\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Getting the true classes\n",
    "iterator = iter(valid_dataset)\n",
    "\n",
    "y_valid = np.zeros((320, 20))\n",
    "i = 0\n",
    "for _ in range(len(valid_gen)):\n",
    "    print(i)\n",
    "    augmented_img, target = next(iterator)\n",
    "    for j in range(0, target.shape[0]):\n",
    "        y_valid[i] = target[j]\n",
    "        i+=1\n",
    "\n",
    "y_valid = y_valid[0:316]\n",
    "y_valid = np.argmax(y_valid, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        19\n",
      "           1       0.89      0.67      0.76        12\n",
      "           2       0.77      1.00      0.87        20\n",
      "           3       0.94      1.00      0.97        16\n",
      "           4       0.91      1.00      0.95        20\n",
      "           5       1.00      0.86      0.92        14\n",
      "           6       1.00      0.81      0.90        16\n",
      "           7       0.88      1.00      0.94        15\n",
      "           8       1.00      0.93      0.97        15\n",
      "           9       0.92      0.92      0.92        12\n",
      "          10       1.00      0.95      0.97        20\n",
      "          11       0.93      0.93      0.93        14\n",
      "          12       0.88      1.00      0.93        14\n",
      "          13       1.00      0.92      0.96        12\n",
      "          14       0.92      0.80      0.86        15\n",
      "          15       1.00      0.85      0.92        20\n",
      "          16       1.00      1.00      1.00        16\n",
      "          17       0.93      1.00      0.97        14\n",
      "          18       1.00      1.00      1.00        12\n",
      "          19       0.91      1.00      0.95        20\n",
      "\n",
      "    accuracy                           0.94       316\n",
      "   macro avg       0.94      0.93      0.93       316\n",
      "weighted avg       0.94      0.94      0.94       316\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_valid, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}